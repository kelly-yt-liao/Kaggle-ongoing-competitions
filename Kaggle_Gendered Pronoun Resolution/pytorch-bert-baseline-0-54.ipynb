{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eef73f480afe9a4b485f89361916b7469ed986ea"
   },
   "source": [
    "Looks like Matei Ionita beated me to it. Check out his kernel [here](https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline). \n",
    "\n",
    "I haven't read his kernel carefully yet, but it seems we have basically the same model design. However, his kernel uses Tensorflow and this one uses PyTorch.\n",
    "\n",
    "Since Matei Ionita alread did a great job explaining the workflow, I won't repeat the same thing here. I'll just let the code do the speaking and comment when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/maggieliao/Kaggle_ongoing_competitions/Kaggle_Gendered Pronoun Resolution'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-development.tsv -q\n",
    "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-test.tsv -q\n",
    "!wget https://github.com/google-research-datasets/gap-coreference/raw/master/gap-validation.tsv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98b461a0d1e2d27558502f9caefeaf7e47871efc"
   },
   "source": [
    "\"pytorch_helper_bot\" is a thin abstraction of some common PyTorch training routines. It can easily be replaced, so you can mostly ignore it and focus on the preprocessing and model definition instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 3.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.0.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.106)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2018.1.10)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.16.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.21.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.31.1)\r\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.106 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.106)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (0.14)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (2.6.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.106->boto3->pytorch-pretrained-bert) (1.12.0)\r\n",
      "Installing collected packages: pytorch-pretrained-bert\r\n",
      "Successfully installed pytorch-pretrained-bert-0.6.1\r\n",
      "Collecting https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\r\n",
      "  Downloading https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip\r\n",
      "\u001b[K     \\ 102kB 7.1MB/s\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from PyTorchHelperBot==0.0.4) (1.0.0)\r\n",
      "Building wheels for collected packages: PyTorchHelperBot\r\n",
      "  Building wheel for PyTorchHelperBot (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-d9wuq8ne/wheels/1f/01/01/da39a14e8e30666f3eec7106664e59059789c330a11b5fa357\r\n",
      "Successfully built PyTorchHelperBot\r\n",
      "Installing collected packages: PyTorchHelperBot\r\n",
      "Successfully installed PyTorchHelperBot-0.0.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert\n",
    "!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.4.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4e128e4337fd5c906540c112bc1d4e0fd2f38ef3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This variable is used by helperbot to make the training deterministic\n",
    "os.environ[\"SEED\"] = \"33223\"\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "\n",
    "from helperbot import BaseBot, TriangularLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4209e502d9d0c58575d71a7580cabc66bbf7ff70"
   },
   "outputs": [],
   "source": [
    "BERT_MODEL = 'bert-large-uncased'\n",
    "CASED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "6d3c64ff3a19456ee88ef77825b83690e5907475"
   },
   "outputs": [],
   "source": [
    "def insert_tag(row):\n",
    "    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n",
    "    to_be_inserted = sorted([\n",
    "        (row[\"A-offset\"], \" [A] \"),\n",
    "        (row[\"B-offset\"], \" [B] \"),\n",
    "        (row[\"Pronoun-offset\"], \" [P] \")\n",
    "    ], key=lambda x: x[0], reverse=True)\n",
    "    text = row[\"Text\"]\n",
    "    for offset, tag in to_be_inserted:\n",
    "        text = text[:offset] + tag + text[offset:]\n",
    "    return text\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n",
    "    entries = {}\n",
    "    final_tokens = []\n",
    "    for token in tokenizer.tokenize(text):\n",
    "        if token in (\"[A]\", \"[B]\", \"[P]\"):\n",
    "            entries[token] = len(final_tokens)\n",
    "            continue\n",
    "        final_tokens.append(token)\n",
    "    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n",
    "\n",
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, labeled=True):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n",
    "            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n",
    "            self.y = tmp.values.astype(\"bool\")\n",
    "        # Extracts the tokens and offsets(positions of A, B, and P)\n",
    "        self.offsets, self.tokens = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            text = insert_tag(row)\n",
    "            tokens, offsets = tokenize(text, tokenizer)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None\n",
    "    \n",
    "def collate_examples(batch, truncate_len=500):\n",
    "    \"\"\"Batch preparation.\n",
    "    \n",
    "    1. Pad the sequences\n",
    "    2. Transform the target.\n",
    "    \"\"\"\n",
    "    transposed = list(zip(*batch))\n",
    "    max_len = min(\n",
    "        max((len(x) for x in transposed[0])),\n",
    "        truncate_len\n",
    "    )\n",
    "    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "    for i, row in enumerate(transposed[0]):\n",
    "        row = np.array(row[:truncate_len])\n",
    "        tokens[i, :len(row)] = row\n",
    "    token_tensor = torch.from_numpy(tokens)\n",
    "    # Offsets\n",
    "    offsets = torch.stack([\n",
    "        torch.LongTensor(x) for x in transposed[1]\n",
    "    ], dim=0) + 1 # Account for the [CLS] token\n",
    "    # Labels\n",
    "    if len(transposed) == 2:\n",
    "        return token_tensor, offsets, None\n",
    "    one_hot_labels = torch.stack([\n",
    "        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n",
    "    ], dim=0)\n",
    "    _, labels = one_hot_labels.max(dim=1)\n",
    "    return token_tensor, offsets, labels\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(bert_hidden_size * 3),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(bert_hidden_size * 3, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "        for i, module in enumerate(self.fc):\n",
    "            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                print(\"Initing batchnorm\")\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if getattr(module, \"weight_v\", None) is not None:\n",
    "                    nn.init.uniform_(module.weight_g, 0, 1)\n",
    "                    nn.init.kaiming_normal_(module.weight_v)\n",
    "                    print(\"Initing linear with weight normalization\")\n",
    "                    assert model[i].weight_g is not None\n",
    "                else:\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    print(\"Initing linear\")\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        assert bert_outputs.size(2) == self.bert_hidden_size\n",
    "        extracted_outputs = bert_outputs.gather(\n",
    "            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n",
    "        ).view(bert_outputs.size(0), -1)\n",
    "        return self.fc(extracted_outputs)\n",
    "\n",
    "    \n",
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device)\n",
    "        self.head = Head(self.bert_hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, token_tensor, offsets):\n",
    "        token_tensor = token_tensor.to(self.device)\n",
    "        bert_outputs, _ =  self.bert(\n",
    "            token_tensor, attention_mask=(token_tensor > 0).long(), \n",
    "            token_type_ids=None, output_all_encoded_layers=False)\n",
    "        head_outputs = self.head(bert_outputs, offsets.to(self.device))\n",
    "        return head_outputs            \n",
    "\n",
    "    \n",
    "def children(m):\n",
    "    return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "\n",
    "def set_trainable_attr(m, b):\n",
    "    m.trainable = b\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = b\n",
    "\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module):\n",
    "        f(m)\n",
    "    if len(c) > 0:\n",
    "        for l in c:\n",
    "            apply_leaf(l, f)\n",
    "\n",
    "            \n",
    "def set_trainable(l, b):\n",
    "    apply_leaf(l, lambda m: set_trainable_attr(m, b))\n",
    "    \n",
    "    \n",
    "class GAPBot(BaseBot):\n",
    "    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n",
    "        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n",
    "        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n",
    "        device=\"cuda:0\", use_tensorboard=False):\n",
    "        super().__init__(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer=optimizer, clip_grad=clip_grad,\n",
    "            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n",
    "            batch_idx=batch_idx, echo=echo,\n",
    "            device=device, use_tensorboard=use_tensorboard\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.loss_format = \"%.6f\"\n",
    "        \n",
    "    def extract_prediction(self, tensor):\n",
    "        return tensor\n",
    "    \n",
    "    def snapshot(self):\n",
    "        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n",
    "        loss = self.eval(self.val_loader)\n",
    "        loss_str = self.loss_format % loss\n",
    "        self.logger.info(\"Snapshot loss %s\", loss_str)\n",
    "        self.logger.tb_scalars(\n",
    "            \"losses\", {\"val\": loss},  self.step)\n",
    "        target_path = (\n",
    "            self.checkpoint_dir / \"best.pth\")        \n",
    "        if not self.best_performers or (self.best_performers[0][0] > loss):\n",
    "            torch.save(self.model.state_dict(), target_path)\n",
    "            self.best_performers = [(loss, target_path, self.step)]\n",
    "        self.logger.info(\"Saving checkpoint %s...\", target_path)\n",
    "        assert Path(target_path).exists()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d534c85ff69192b4dd1ec670fc9c2b9392cc7a62"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"gap-test.tsv\", delimiter=\"\\t\")\n",
    "df_val = pd.read_csv(\"gap-validation.tsv\", delimiter=\"\\t\")\n",
    "df_test = pd.read_csv(\"gap-development.tsv\", delimiter=\"\\t\")\n",
    "sample_sub = pd.read_csv(\"../input/sample_submission_stage_1.csv\")\n",
    "assert sample_sub.shape[0] == df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "d58d18c34f5df9ec8f8d8fb048ae6c10fbf9914a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 5981550.07B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    BERT_MODEL,\n",
    "    do_lower_case=CASED,\n",
    "    never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n",
    ")\n",
    "# These tokens are not actually used, so we can assign arbitrary values.\n",
    "tokenizer.vocab[\"[A]\"] = -1\n",
    "tokenizer.vocab[\"[B]\"] = -1\n",
    "tokenizer.vocab[\"[P]\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "69689365738454b33649a14d83eea49cc1b18687"
   },
   "outputs": [],
   "source": [
    "train_ds = GAPDataset(df_train, tokenizer)\n",
    "val_ds = GAPDataset(df_val, tokenizer)\n",
    "test_ds = GAPDataset(df_test, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=20,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    collate_fn = collate_examples,\n",
    "    batch_size=128,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "a85f87ed8d73b520e39a5dc07da1838867ac2653"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248501532/1248501532 [00:27<00:00, 44605242.37B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing batchnorm\n",
      "Initing linear\n",
      "Initing batchnorm\n",
      "Initing linear\n"
     ]
    }
   ],
   "source": [
    "model = GAPModel(BERT_MODEL, torch.device(\"cuda:0\"))\n",
    "# You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n",
    "set_trainable(model.bert, False)\n",
    "set_trainable(model.head, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "493b0ed0887339dfe818df1a0be17c05a4c97d17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/06/2019 03:20:55 AM]] SEED: 33223\n",
      "[[03/06/2019 03:20:55 AM]] # of paramters: 336,723,971\n",
      "[[03/06/2019 03:20:55 AM]] # of trainable paramters: 1,582,083\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "bot = GAPBot(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer=optimizer, echo=True,\n",
    "    avg_window=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "07ea447ea766df3d997779e6c9a8300b7532a049"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[03/06/2019 03:20:55 AM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[03/06/2019 03:20:55 AM]] Batches per epoch: 100\n",
      "[[03/06/2019 03:20:55 AM]] ====================Epoch 1====================\n",
      "[[03/06/2019 03:21:30 AM]] Step 25: train 1.795615 lr: 1.816e-04\n",
      "[[03/06/2019 03:22:07 AM]] Step 50: train 1.677933 lr: 3.247e-04\n",
      "[[03/06/2019 03:22:42 AM]] Step 75: train 1.553931 lr: 4.678e-04\n",
      "[[03/06/2019 03:23:17 AM]] Step 100: train 1.489162 lr: 6.108e-04\n",
      "100%|██████████| 4/4 [00:36<00:00,  8.51s/it]\n",
      "[[03/06/2019 03:23:54 AM]] Snapshot loss 0.907547\n",
      "[[03/06/2019 03:23:57 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/06/2019 03:23:57 AM]] New low\n",
      "\n",
      "[[03/06/2019 03:23:57 AM]] ====================Epoch 2====================\n",
      "[[03/06/2019 03:24:34 AM]] Step 125: train 1.437149 lr: 7.539e-04\n",
      "[[03/06/2019 03:25:09 AM]] Step 150: train 1.375977 lr: 8.970e-04\n",
      "[[03/06/2019 03:25:42 AM]] Step 175: train 1.333890 lr: 9.801e-04\n",
      "[[03/06/2019 03:26:18 AM]] Step 200: train 1.306729 lr: 9.090e-04\n",
      "100%|██████████| 4/4 [00:36<00:00,  8.53s/it]\n",
      "[[03/06/2019 03:26:55 AM]] Snapshot loss 0.705873\n",
      "[[03/06/2019 03:26:57 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/06/2019 03:26:57 AM]] New low\n",
      "\n",
      "[[03/06/2019 03:26:57 AM]] ====================Epoch 3====================\n",
      "[[03/06/2019 03:27:28 AM]] Step 225: train 1.248140 lr: 8.379e-04\n",
      "[[03/06/2019 03:28:06 AM]] Step 250: train 1.205883 lr: 7.668e-04\n",
      "[[03/06/2019 03:28:39 AM]] Step 275: train 1.173786 lr: 6.957e-04\n",
      "[[03/06/2019 03:29:15 AM]] Step 300: train 1.141156 lr: 6.246e-04\n",
      "100%|██████████| 4/4 [00:36<00:00,  8.54s/it]\n",
      "[[03/06/2019 03:29:51 AM]] Snapshot loss 0.709880\n",
      "[[03/06/2019 03:29:51 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/06/2019 03:29:51 AM]] ====================Epoch 4====================\n",
      "[[03/06/2019 03:30:24 AM]] Step 325: train 1.051027 lr: 5.534e-04\n",
      "[[03/06/2019 03:31:01 AM]] Step 350: train 0.975062 lr: 4.823e-04\n",
      "[[03/06/2019 03:31:36 AM]] Step 375: train 0.922527 lr: 4.112e-04\n",
      "[[03/06/2019 03:32:11 AM]] Step 400: train 0.873056 lr: 3.401e-04\n",
      "100%|██████████| 4/4 [00:36<00:00,  8.52s/it]\n",
      "[[03/06/2019 03:32:48 AM]] Snapshot loss 0.682720\n",
      "[[03/06/2019 03:32:50 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/06/2019 03:32:50 AM]] New low\n",
      "\n",
      "[[03/06/2019 03:32:50 AM]] ====================Epoch 5====================\n",
      "[[03/06/2019 03:33:23 AM]] Step 425: train 0.824748 lr: 2.690e-04\n",
      "[[03/06/2019 03:34:00 AM]] Step 450: train 0.788236 lr: 1.979e-04\n",
      "[[03/06/2019 03:34:36 AM]] Step 475: train 0.747537 lr: 1.268e-04\n",
      "[[03/06/2019 03:35:09 AM]] Step 500: train 0.708095 lr: 5.569e-05\n",
      "100%|██████████| 4/4 [00:36<00:00,  8.54s/it]\n",
      "[[03/06/2019 03:35:46 AM]] Snapshot loss 0.644455\n",
      "[[03/06/2019 03:35:48 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[03/06/2019 03:35:48 AM]] New low\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_loader) \n",
    "n_steps = steps_per_epoch * 5\n",
    "bot.train(\n",
    "    n_steps,\n",
    "    log_interval=steps_per_epoch // 4,\n",
    "    snapshot_interval=steps_per_epoch,\n",
    "    scheduler=TriangularLR(\n",
    "        optimizer, 20, ratio=2, steps_per_cycle=n_steps)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d92655729f678dcbe93a5ee824562b0d23fe275f"
   },
   "outputs": [],
   "source": [
    "# Load the best checkpoint\n",
    "bot.load_model(bot.best_performers[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "efad23ae0f411fe486e837be0903dd24cab6cba5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:49<00:00,  8.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5382917318344116"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on the test dataset\n",
    "bot.eval(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e3b1bc4a4264ebda30d4eca35879f1df6f0a11c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:48<00:00,  8.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract predictions to the test dataset\n",
    "preds = bot.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "51b7e2335c8f9c1b821b6aeaba7c5122fe74530a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900758</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.030333</td>\n",
       "      <td>development-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997632</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>development-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020568</td>\n",
       "      <td>0.725555</td>\n",
       "      <td>0.253877</td>\n",
       "      <td>development-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026126</td>\n",
       "      <td>0.859066</td>\n",
       "      <td>0.114808</td>\n",
       "      <td>development-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.977662</td>\n",
       "      <td>0.017476</td>\n",
       "      <td>development-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B   NEITHER             ID\n",
       "0  0.900758  0.068909  0.030333  development-1\n",
       "1  0.997632  0.001000  0.001960  development-2\n",
       "2  0.020568  0.725555  0.253877  development-3\n",
       "3  0.026126  0.859066  0.114808  development-4\n",
       "4  0.004862  0.977662  0.017476  development-5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create submission file\n",
    "df_sub = pd.DataFrame(torch.softmax(preds, -1).cpu().numpy().clip(1e-3, 1-1e-3), columns=[\"A\", \"B\", \"NEITHER\"])\n",
    "df_sub[\"ID\"] = df_test.ID\n",
    "df_sub.to_csv(\"cache/sub.csv\", index=False)\n",
    "df_sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_venv",
   "language": "python",
   "name": "py36_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
